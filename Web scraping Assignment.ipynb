{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de01f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  requests\n",
    "from  bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d1245c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all the header tags :\n",
      "\n",
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\">Main Page</h1>\n",
      "\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n",
      "\n",
      "<h2>Navigation menu</h2>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-personal-label\">\n",
      "<span class=\"vector-menu-heading-label\">Personal tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-namespaces-label\">\n",
      "<span class=\"vector-menu-heading-label\">Namespaces</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-views-label\">\n",
      "<span class=\"vector-menu-heading-label\">Views</span>\n",
      "</h3>\n",
      "\n",
      "<h3>\n",
      "<label for=\"searchInput\">Search</label>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-navigation-label\">\n",
      "<span class=\"vector-menu-heading-label\">Navigation</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-interaction-label\">\n",
      "<span class=\"vector-menu-heading-label\">Contribute</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-tb-label\">\n",
      "<span class=\"vector-menu-heading-label\">Tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-coll-print_export-label\">\n",
      "<span class=\"vector-menu-heading-label\">Print/export</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-wikibase-otherprojects-label\">\n",
      "<span class=\"vector-menu-heading-label\">In other projects</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-lang-label\">\n",
      "<span class=\"vector-menu-heading-label\">Languages</span>\n",
      "</h3>\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to display all the header tags from wikipedia.org.\n",
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "def wikipedia_headers(url):\n",
    "   \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    titles = soup.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "    print('List all the header tags :', *titles, sep='\\n\\n')\n",
    "    return\n",
    "\n",
    "wikipedia_headers(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce09292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Year of release</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>(1972)</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>(2008)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>(1974)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>(1957)</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Jagten</td>\n",
       "      <td>(2012)</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>M - Eine Stadt sucht einen Mörder</td>\n",
       "      <td>(1931)</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>(1959)</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Vertigo</td>\n",
       "      <td>(1958)</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Le fabuleux destin d'Amélie Poulain</td>\n",
       "      <td>(2001)</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Movie Name Year of release Rating\n",
       "0              The Shawshank Redemption          (1994)    9.2\n",
       "1                         The Godfather          (1972)    9.2\n",
       "2                       The Dark Knight          (2008)    9.0\n",
       "3                 The Godfather Part II          (1974)    9.0\n",
       "4                          12 Angry Men          (1957)    8.9\n",
       "..                                  ...             ...    ...\n",
       "95                               Jagten          (2012)    8.3\n",
       "96    M - Eine Stadt sucht einen Mörder          (1931)    8.3\n",
       "97                   North by Northwest          (1959)    8.3\n",
       "98                              Vertigo          (1958)    8.2\n",
       "99  Le fabuleux destin d'Amélie Poulain          (2001)    8.2\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame. \n",
    "url = \"https://www.imdb.com/chart/top/?ref_=nv_mv_250\"\n",
    "def IMDB_Movie(url):    \n",
    "    \n",
    "\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "    titles = soup.find_all(\"td\",{\"class\":\"titleColumn\"})[0:100]\n",
    "\n",
    "    ratings = soup.find_all(\"td\",class_=\"ratingColumn imdbRating\")[0:100]\n",
    "\n",
    "    movie_rating = []\n",
    "    for i in ratings:\n",
    "        movie_rating.append(i.text.replace('\\n',''))\n",
    "\n",
    "    year =soup.find_all('span',class_=\"secondaryInfo\")[0:100]\n",
    "\n",
    "    movie_name=[]\n",
    "    movie_year=[]\n",
    "    for i in titles:\n",
    "        movie_name.append(i.a.text)\n",
    "    for i in year:\n",
    "        movie_year.append(i.text)\n",
    "\n",
    "    data = list(zip(movie_name,movie_year,movie_rating))\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data,columns =[\"Movie Name\",\"Year of release\",\"Rating\"])\n",
    "\n",
    "    df\n",
    "    return(df)\n",
    "IMDB_Movie(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed292147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Year of release</th>\n",
       "      <th>IMDB rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anbe Sivam</td>\n",
       "      <td>(2003)</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jai Bhim</td>\n",
       "      <td>(2021)</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Golmaal</td>\n",
       "      <td>(1979)</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nayakan</td>\n",
       "      <td>(1987)</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pariyerum Perumal</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Baasha</td>\n",
       "      <td>(1995)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Masaan</td>\n",
       "      <td>(2015)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Baahubali 2: The Conclusion</td>\n",
       "      <td>(2017)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Kahaani</td>\n",
       "      <td>(2012)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Dil Chahta Hai</td>\n",
       "      <td>(2001)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Movie Name Year of release IMDB rating\n",
       "0                    Anbe Sivam          (2003)         8.4\n",
       "1                      Jai Bhim          (2021)         8.4\n",
       "2                       Golmaal          (1979)         8.4\n",
       "3                       Nayakan          (1987)         8.4\n",
       "4             Pariyerum Perumal          (2018)         8.4\n",
       "..                          ...             ...         ...\n",
       "95                       Baasha          (1995)         8.0\n",
       "96                       Masaan          (2015)         8.0\n",
       "97  Baahubali 2: The Conclusion          (2017)         8.0\n",
       "98                      Kahaani          (2012)         8.0\n",
       "99               Dil Chahta Hai          (2001)         8.0\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year ofrelease) and make data frame.\n",
    "def IMDB_HINDI_MOVIES(url):\n",
    "    url = \"https://www.imdb.com/india/top-rated-indian-movies/?pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=461131e5-5af0-4e50-bee2-223fad1e00ca&pf_rd_r=2K1DA1PEPHHTHT10Y16D&pf_rd_s=center-1&pf_rd_t=60601&pf_rd_i=india.toprated&ref_=fea_india_ss_toprated_india_tr_india250_sm\"\n",
    "\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "    movie_rating = []\n",
    "    for i in soup.find_all(\"td\",class_=\"ratingColumn imdbRating\"):\n",
    "        movie_rating.append(i.text.replace('\\n',''))\n",
    "\n",
    "    m_name=[]\n",
    "    for i in soup.find_all(\"td\",{\"class\":\"titleColumn\"}):\n",
    "        m_name.append(i.a.text)\n",
    "\n",
    "    m_year=[]\n",
    "    for i in soup.find_all('span',class_=\"secondaryInfo\"):\n",
    "        m_year.append(i.text)\n",
    "\n",
    "    data = list(zip(m_name,m_year,movie_rating))\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data,columns =[\"Movie Name\",\"Year of release\",\"IMDB rating\"])\n",
    "    return(df.head(100))\n",
    "IMDB_HINDI_MOVIES(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a2731a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Former Presidents</th>\n",
       "      <th>Term of Office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Pranab Mukherjee</td>\n",
       "      <td>25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Smt Pratibha Devisingh Patil</td>\n",
       "      <td>25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam</td>\n",
       "      <td>25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shri K. R. Narayanan</td>\n",
       "      <td>25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dr Shankar Dayal Sharma</td>\n",
       "      <td>25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shri R Venkataraman</td>\n",
       "      <td>25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Giani Zail Singh</td>\n",
       "      <td>25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy</td>\n",
       "      <td>25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed</td>\n",
       "      <td>24 August, 1974 to 11 February, 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Shri Varahagiri Venkata Giri</td>\n",
       "      <td>3 May, 1969 to 20 July, 1969 and 24 August, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Dr. Zakir Husain</td>\n",
       "      <td>13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan</td>\n",
       "      <td>13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Rajendra Prasad</td>\n",
       "      <td>26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Former Presidents  \\\n",
       "0          Shri Pranab Mukherjee    \n",
       "1   Smt Pratibha Devisingh Patil    \n",
       "2         DR. A.P.J. Abdul Kalam    \n",
       "3           Shri K. R. Narayanan    \n",
       "4        Dr Shankar Dayal Sharma    \n",
       "5            Shri R Venkataraman    \n",
       "6               Giani Zail Singh    \n",
       "7      Shri Neelam Sanjiva Reddy    \n",
       "8       Dr. Fakhruddin Ali Ahmed    \n",
       "9   Shri Varahagiri Venkata Giri    \n",
       "10              Dr. Zakir Husain    \n",
       "11  Dr. Sarvepalli Radhakrishnan    \n",
       "12           Dr. Rajendra Prasad    \n",
       "\n",
       "                                       Term of Office  \n",
       "0                     25 July, 2012 to 25 July, 2017   \n",
       "1                     25 July, 2007 to 25 July, 2012   \n",
       "2                     25 July, 2002 to 25 July, 2007   \n",
       "3                     25 July, 1997 to 25 July, 2002   \n",
       "4                     25 July, 1992 to 25 July, 1997   \n",
       "5                     25 July, 1987 to 25 July, 1992   \n",
       "6                     25 July, 1982 to 25 July, 1987   \n",
       "7                     25 July, 1977 to 25 July, 1982   \n",
       "8                24 August, 1974 to 11 February, 1977  \n",
       "9    3 May, 1969 to 20 July, 1969 and 24 August, 1...  \n",
       "10                        13 May, 1967 to 3 May, 1969  \n",
       "11                       13 May, 1962 to 13 May, 1967  \n",
       "12                   26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write s python program to display list of respected former presidents of India(i.e. Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests as rq\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "page = rq.get(url)\n",
    "soup = bs(page.content, 'html.parser')\n",
    "scraped_names = soup.find_all('div', class_=\"presidentListing\")\n",
    "names = []\n",
    "for name in scraped_names:\n",
    "    names.append(name.find('h3').text.split('(')[0])\n",
    "scraped_tops = soup.find_all('div', class_=\"presidentListing\")\n",
    "tops = []\n",
    "for top in scraped_tops:\n",
    "    tops.append(top.find('p').text.split(':')[1])\n",
    "data = pd.DataFrame()\n",
    "data['Former Presidents'] = names\n",
    "data['Term of Office'] = tops\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f06d15b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team Name</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>12</td>\n",
       "      <td>1,505</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>England</td>\n",
       "      <td>22</td>\n",
       "      <td>2,756</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>19</td>\n",
       "      <td>2,005</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>22</td>\n",
       "      <td>2,304</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Australia</td>\n",
       "      <td>23</td>\n",
       "      <td>2,325</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>19</td>\n",
       "      <td>1,872</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>24</td>\n",
       "      <td>2,275</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>29</td>\n",
       "      <td>2,658</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>32</td>\n",
       "      <td>2,306</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>18</td>\n",
       "      <td>1,238</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Team Name Matches Points Ratings\n",
       "0   New Zealand      12  1,505     125\n",
       "1       England      22  2,756     125\n",
       "2      Pakistan      19  2,005     106\n",
       "3         India      22  2,304     105\n",
       "4     Australia      23  2,325     101\n",
       "5  South Africa      19  1,872      99\n",
       "6    Bangladesh      24  2,275      95\n",
       "7     Sri Lanka      29  2,658      92\n",
       "8   West Indies      32  2,306      72\n",
       "9   Afghanistan      18  1,238      69"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "# b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "# c) Top 10 ODI bowlers along with the records of their team and rating.\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "def Top_Mens_Teams(url):\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "    table = soup.find(\"tbody\")\n",
    "\n",
    "    team_name = table.find_all(\"span\",class_=\"u-hide-phablet\")[0:10]\n",
    "\n",
    "    teams = []\n",
    "    for i in team_name:\n",
    "        teams.append(i.text)\n",
    "\n",
    "    match =  table.find_all(\"td\",class_=\"table-body__cell u-center-text\")\n",
    "\n",
    "    f_match = table.find(\"td\",class_=\"rankings-block__banner--matches\").text\n",
    "\n",
    "    f_points = table.find(\"td\",class_=\"rankings-block__banner--points\").text\n",
    "\n",
    "    matches = [f_match,f_points]\n",
    "    for i in match:\n",
    "        matches.append(i.text)\n",
    "\n",
    "    m_records = matches[0:20:2]\n",
    "\n",
    "    m_points = matches[1:20:2]\n",
    "\n",
    "    f_rating = table.find(\"td\",class_=\"rankings-block__banner--rating u-text-right\").text.replace('\\n','').strip()\n",
    "\n",
    "    ratings = table.find_all(\"td\",class_=\"table-body__cell u-text-right rating\")[0:9]\n",
    "\n",
    "    team_rating = [f_rating]\n",
    "    for i in ratings:\n",
    "        team_rating.append(i.text)\n",
    "\n",
    "    data = list(zip(teams,m_records,m_points,team_rating))\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data, columns = [\"Team Name\",\"Matches\",\"Points\",\"Ratings\"])\n",
    "    return(df)\n",
    "Top_Mens_Teams(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a472e965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player Name</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Imam-ul-Haq</td>\n",
       "      <td>PAK</td>\n",
       "      <td>815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ross Taylor</td>\n",
       "      <td>NZ</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rassie van der Dussen</td>\n",
       "      <td>SA</td>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jonny Bairstow</td>\n",
       "      <td>ENG</td>\n",
       "      <td>752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Shai Hope</td>\n",
       "      <td>WI</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Player Name Team Rating\n",
       "0             Babar Azam  PAK    892\n",
       "1            Imam-ul-Haq  PAK    815\n",
       "2            Virat Kohli  IND    811\n",
       "3           Rohit Sharma  IND    791\n",
       "4        Quinton de Kock   SA    789\n",
       "5            Ross Taylor   NZ    775\n",
       "6  Rassie van der Dussen   SA    769\n",
       "7         Jonny Bairstow  ENG    752\n",
       "8           David Warner  AUS    737\n",
       "9              Shai Hope   WI    718"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "def Top_Batsmen(url):    \n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\",class_=\"table rankings-table\")\n",
    "\n",
    "    first_player = table.find(\"div\",class_=\"rankings-block__banner--name-large\").text\n",
    "\n",
    "    first_team = table.find(\"div\",class_=\"rankings-block__banner--nationality\").text.replace('\\n','')\n",
    "\n",
    "    first_rating = table.find(\"div\",class_=\"rankings-block__banner--rating\").text\n",
    "\n",
    "    players = table.find_all(\"td\",class_=\"table-body__cell rankings-table__name name\")[0:9]\n",
    "\n",
    "    player = [first_player]\n",
    "    for i in players:\n",
    "        player.append(i.text.replace('\\n',''))\n",
    "\n",
    "    teams= table.find_all(\"span\",class_=\"table-body__logo-text\")[0:9]\n",
    "\n",
    "    team = [first_team]\n",
    "    for i in teams:\n",
    "        team.append(i.text)\n",
    "\n",
    "    ratings = table.find_all(\"td\",class_=\"table-body__cell rating\")[0:9]\n",
    "\n",
    "    rating = [first_rating]\n",
    "    for i in ratings:\n",
    "        rating.append(i.text)\n",
    "\n",
    "    data = list(zip(player,team,rating))\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data, columns = [\"Player Name\",\"Team\",\"Rating\"])\n",
    "    return(df)\n",
    "Top_Batsmen(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e9c77f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bowler Name</th>\n",
       "      <th>Team</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Matt Henry</td>\n",
       "      <td>NZ</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shaheen Afridi</td>\n",
       "      <td>PAK</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chris Woakes</td>\n",
       "      <td>ENG</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jasprit Bumrah</td>\n",
       "      <td>IND</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mehedi Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mohammad Nabi</td>\n",
       "      <td>AFG</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Shakib Al Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Bowler Name Team Ratings\n",
       "0       Trent Boult   NZ     726\n",
       "1        Matt Henry   NZ     683\n",
       "2    Shaheen Afridi  PAK     681\n",
       "3      Chris Woakes  ENG     680\n",
       "4    Jasprit Bumrah  IND     679\n",
       "5    Josh Hazlewood  AUS     679\n",
       "6  Mujeeb Ur Rahman  AFG     676\n",
       "7      Mehedi Hasan  BAN     661\n",
       "8     Mohammad Nabi  AFG     657\n",
       "9   Shakib Al Hasan  BAN     657"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "def Top_Bowlers(url):\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\",class_=\"table rankings-table\")\n",
    "\n",
    "    f_bowler = table.find(\"div\",class_=\"rankings-block__banner--name-large\").text\n",
    "\n",
    "    b_team= table.find(\"div\",class_=\"rankings-block__banner--nationality\").text.replace('\\n','')\n",
    "\n",
    "    b_rating = table.find(\"div\",class_=\"rankings-block__banner--rating\").text\n",
    "\n",
    "    bowlers = table.find_all(\"td\",class_=\"table-body__cell rankings-table__name name\")[0:9]\n",
    "\n",
    "    bowler = [f_bowler]\n",
    "    for i in bowlers:\n",
    "        bowler.append(i.text.replace('\\n',''))\n",
    "\n",
    "    teams = table.find_all(\"span\",class_=\"table-body__logo-text\")[0:9]\n",
    "\n",
    "    team=[b_team]\n",
    "    for i in teams:\n",
    "        team.append(i.text)\n",
    "\n",
    "    ratings = table.find_all(\"td\",class_=\"table-body__cell rating\")[0:9]\n",
    "\n",
    "    rating = [b_rating]\n",
    "    for i in ratings:\n",
    "        rating.append(i.text)\n",
    "\n",
    "    data = list(zip(bowler,team,rating))\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data, columns = [\"Bowler Name\",\"Team\",\"Ratings\"])\n",
    "    return(df)\n",
    "Top_Bowlers(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4127a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team Name</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australia</td>\n",
       "      <td>29</td>\n",
       "      <td>4,837</td>\n",
       "      <td>167               ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>32</td>\n",
       "      <td>3,949</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>England</td>\n",
       "      <td>30</td>\n",
       "      <td>3,531</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>31</td>\n",
       "      <td>3,109</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>31</td>\n",
       "      <td>3,019</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>30</td>\n",
       "      <td>2,768</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>12</td>\n",
       "      <td>930</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>30</td>\n",
       "      <td>1,962</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>10</td>\n",
       "      <td>459</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>8</td>\n",
       "      <td>351</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Team Name Matches Points  \\\n",
       "0     Australia      29  4,837   \n",
       "1  South Africa      32  3,949   \n",
       "2       England      30  3,531   \n",
       "3         India      31  3,109   \n",
       "4   New Zealand      31  3,019   \n",
       "5   West Indies      30  2,768   \n",
       "6    Bangladesh      12    930   \n",
       "7      Pakistan      30  1,962   \n",
       "8     Sri Lanka      10    459   \n",
       "9       Ireland       8    351   \n",
       "\n",
       "                                             Ratings  \n",
       "0                              167               ...  \n",
       "1                                                123  \n",
       "2                                                118  \n",
       "3                                                100  \n",
       "4                                                 97  \n",
       "5                                                 92  \n",
       "6                                                 78  \n",
       "7                                                 65  \n",
       "8                                                 46  \n",
       "9                                                 44  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "# b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "# c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    " \n",
    "url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "def Top_Womens_Team(url):\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "    table = soup.find(\"tbody\")\n",
    "\n",
    "    team_name = table.find_all(\"span\",class_=\"u-hide-phablet\")[0:10]\n",
    "\n",
    "    teams = []\n",
    "    for i in team_name:\n",
    "        teams.append(i.text)\n",
    "\n",
    "    match = table.find(\"td\",class_=\"rankings-block__banner--matches\").text\n",
    "\n",
    "    point = table.find(\"td\",class_=\"rankings-block__banner--points\").text\n",
    "\n",
    "    matches = table.find_all(\"td\",class_=\"table-body__cell u-center-text\")\n",
    "\n",
    "    matches_ = [match,point]\n",
    "    for i in matches:\n",
    "        matches_.append(i.text)\n",
    "\n",
    "    m_record = matches_[0:20:2]\n",
    "\n",
    "    m_points = matches_[1:20:2]\n",
    "\n",
    "    f_rating = table.find(\"td\",class_=\"rankings-block__banner--rating u-text-right\").text.replace('\\n','')\n",
    "\n",
    "    ratings = table.find_all(\"td\",class_=\"table-body__cell u-text-right rating\")\n",
    "\n",
    "    rating = [f_rating]\n",
    "    for i in ratings:\n",
    "        rating.append(i.text)\n",
    "\n",
    "    data = list(zip(teams,m_record,m_points,rating))\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data,columns = [\"Team Name\",\"Matches\",\"Points\",\"Ratings\"])\n",
    "    return(df)\n",
    "Top_Womens_Team(url)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d9e7a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bowler Name</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rachael Haynes</td>\n",
       "      <td>AUS</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tammy Beaumont</td>\n",
       "      <td>ENG</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chamari Athapaththu</td>\n",
       "      <td>SL</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Bowler Name Team Rating\n",
       "0         Alyssa Healy  AUS    785\n",
       "1       Natalie Sciver  ENG    750\n",
       "2          Beth Mooney  AUS    749\n",
       "3      Laura Wolvaardt   SA    714\n",
       "4          Meg Lanning  AUS    710\n",
       "5       Rachael Haynes  AUS    701\n",
       "6    Amy Satterthwaite   NZ    681\n",
       "7      Smriti Mandhana  IND    671\n",
       "8       Tammy Beaumont  ENG    660\n",
       "9  Chamari Athapaththu   SL    643"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "def Top_Women_Players(url):\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.text,\"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\",class_=\"table rankings-table\")\n",
    "\n",
    "    first_player = table.find(\"div\",class_=\"rankings-block__banner--name-large\").text\n",
    "\n",
    "    first_team = table.find(\"div\",class_=\"rankings-block__banner--nationality\").text.replace('\\n','')\n",
    "\n",
    "    first_rating= table.find(\"div\",class_=\"rankings-block__banner--rating\").text\n",
    "\n",
    "    w_players = table.find_all(\"td\",class_=\"table-body__cell rankings-table__name name\")[0:9]\n",
    "\n",
    "    w_player = [first_player]\n",
    "    for i in w_players:\n",
    "        w_player.append(i.text.replace('\\n',''))\n",
    "\n",
    "    w_teams = table.find_all(\"span\",class_=\"table-body__logo-text\")[0:9]\n",
    "\n",
    "    w_team = [first_team]\n",
    "    for i in w_teams:\n",
    "        w_team.append(i.text)\n",
    "\n",
    "    w_ratings = table.find_all(\"td\",class_=\"table-body__cell rating\")[0:9]\n",
    "\n",
    "    w_rating = [first_rating]\n",
    "    for i in w_ratings:\n",
    "        w_rating.append(i.text)\n",
    "\n",
    "    data = list(zip(w_player,w_team,w_rating))\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data, columns = [\"Bowler Name\",\"Team\",\"Rating\"])\n",
    "    return(df)\n",
    "Top_Women_Players(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1694ca6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Allrounder Name</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>WI</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amelia Kerr</td>\n",
       "      <td>NZ</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ashleigh Gardner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sune Luus</td>\n",
       "      <td>SA</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Katherine Brunt</td>\n",
       "      <td>ENG</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Allrounder Name Team Rating\n",
       "0    Natalie Sciver  ENG    393\n",
       "1      Ellyse Perry  AUS    374\n",
       "2   Hayley Matthews   WI    339\n",
       "3    Marizanne Kapp   SA    338\n",
       "4       Amelia Kerr   NZ    336\n",
       "5  Ashleigh Gardner  AUS    270\n",
       "6     Deepti Sharma  IND    269\n",
       "7     Jess Jonassen  AUS    246\n",
       "8         Sune Luus   SA    223\n",
       "9   Katherine Brunt  ENG    221"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "def Womens_Allrounder(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text,\"html.parser\")\n",
    "    a_table = soup.find(\"table\",class_=\"table rankings-table\")\n",
    "    f_allrounder = a_table.find(\"div\",class_=\"rankings-block__banner--name-large\").text\n",
    "    a_team = a_table.find(\"div\",class_=\"rankings-block__banner--nationality\").text.replace('\\n','')\n",
    "    a_rating = a_table.find(\"div\",class_=\"rankings-block__banner--rating\").text\n",
    "    allrounders = a_table.find_all(\"td\",class_=\"table-body__cell rankings-table__name name\")[0:9]\n",
    "    allrounder = [f_allrounder]\n",
    "    for i in allrounders:\n",
    "        allrounder.append(i.text.replace('\\n',''))\n",
    "    all_teams = a_table.find_all(\"span\",class_=\"table-body__logo-text\")[0:9]\n",
    "    all_team = [a_team]\n",
    "    for i in all_teams:\n",
    "        all_team.append(i.text)\n",
    "    all_ratings = a_table.find_all(\"td\",class_=\"table-body__cell rating\")[0:9]\n",
    "    all_rating = [a_rating]\n",
    "    for i in all_ratings:\n",
    "        all_rating.append(i.text)\n",
    "    data = list(zip(allrounder,all_team,all_rating))\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data, columns = [\"Allrounder Name\",\"Team\",\"Rating\"])\n",
    "    return(df)\n",
    "Womens_Allrounder(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa3fe088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeadLines</th>\n",
       "      <th>Time</th>\n",
       "      <th>News Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judge throws out Trump-era changes that weaken...</td>\n",
       "      <td>29 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/trump-era-chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Minions' vs. 'Lightyear': Here's why the sill...</td>\n",
       "      <td>36 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/why-minions-be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Washington leaders at odds over proposed tax i...</td>\n",
       "      <td>54 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/social-securit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bridgewater's flagship fund is up more than 30...</td>\n",
       "      <td>55 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/ray-dalio-is-h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Here are top 4 and bottom 4 Club stocks in jus...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/here-are-top-4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Here's how to avoid violating ‘wash sale rules...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/how-investors-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>These stocks can offer protection from next bi...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/these-stocks-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Grand jury investigating Trump election interf...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/georgia-grand-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The 4 most 'recession-proof' industries to wor...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/top-4-recessio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UK's Boris Johnson suffers major blow as finan...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/uk-finance-min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>One boring stock quietly hits all-time high as...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/one-boring-sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Stocks making the biggest moves midday: Oil st...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/stocks-making-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bonds flash recession warning as key part of t...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/bonds-flash-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>With record prices and rising loan costs, luxu...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/luxury-car-buy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Goldman Sachs offers three main strategies to ...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/goldman-sachs-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>We're buying more bank shares as the market se...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/investing-club...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Oil tumbles as much as 10%, breaks below $100 ...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/oil-tumbles-mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How to buy a carbon-conscious fund in wake of ...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/how-to-buy-a-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Italy declared a state of emergency because of...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/italy-declared...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bank of England's Bailey warns global economic...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/bank-of-englan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Oil could plummet to the $60s if a recession h...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/oil-could-plum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Why driving big rig trucks is a job fewer Amer...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/why-driving-bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Germany's much-vaunted trade surplus disappear...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/germanys-much-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Cameron Diaz joins 'unretirement' trend—here's...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/cameron-diaz-j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Yum Brands says it is close to selling its Rus...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/yum-brands-say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Credit Suisse slashes year-end S&amp;P 500 target,...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/credit-suisse-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Buy shares of Crocs, which could climb 50% fro...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/buy-crocs-shar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Downside momentum could mean a new low is ahea...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/top-chart-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Ford reports slight uptick in quarterly sales ...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/ford-f-q2-2022...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>JPMorgan cuts Tesla price target, says shares ...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/jpmorgan-cuts-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            HeadLines         Time  \\\n",
       "0   Judge throws out Trump-era changes that weaken...   29 Min Ago   \n",
       "1   'Minions' vs. 'Lightyear': Here's why the sill...   36 Min Ago   \n",
       "2   Washington leaders at odds over proposed tax i...   54 Min Ago   \n",
       "3   Bridgewater's flagship fund is up more than 30...   55 Min Ago   \n",
       "4   Here are top 4 and bottom 4 Club stocks in jus...   1 Hour Ago   \n",
       "5   Here's how to avoid violating ‘wash sale rules...  2 Hours Ago   \n",
       "6   These stocks can offer protection from next bi...  2 Hours Ago   \n",
       "7   Grand jury investigating Trump election interf...  3 Hours Ago   \n",
       "8   The 4 most 'recession-proof' industries to wor...  3 Hours Ago   \n",
       "9   UK's Boris Johnson suffers major blow as finan...  3 Hours Ago   \n",
       "10  One boring stock quietly hits all-time high as...  3 Hours Ago   \n",
       "11  Stocks making the biggest moves midday: Oil st...  4 Hours Ago   \n",
       "12  Bonds flash recession warning as key part of t...  4 Hours Ago   \n",
       "13  With record prices and rising loan costs, luxu...  5 Hours Ago   \n",
       "14  Goldman Sachs offers three main strategies to ...  5 Hours Ago   \n",
       "15  We're buying more bank shares as the market se...  5 Hours Ago   \n",
       "16  Oil tumbles as much as 10%, breaks below $100 ...  5 Hours Ago   \n",
       "17  How to buy a carbon-conscious fund in wake of ...  5 Hours Ago   \n",
       "18  Italy declared a state of emergency because of...  5 Hours Ago   \n",
       "19  Bank of England's Bailey warns global economic...  6 Hours Ago   \n",
       "20  Oil could plummet to the $60s if a recession h...  6 Hours Ago   \n",
       "21  Why driving big rig trucks is a job fewer Amer...  6 Hours Ago   \n",
       "22  Germany's much-vaunted trade surplus disappear...  6 Hours Ago   \n",
       "23  Cameron Diaz joins 'unretirement' trend—here's...  6 Hours Ago   \n",
       "24  Yum Brands says it is close to selling its Rus...  6 Hours Ago   \n",
       "25  Credit Suisse slashes year-end S&P 500 target,...  7 Hours Ago   \n",
       "26  Buy shares of Crocs, which could climb 50% fro...  7 Hours Ago   \n",
       "27  Downside momentum could mean a new low is ahea...  7 Hours Ago   \n",
       "28  Ford reports slight uptick in quarterly sales ...  7 Hours Ago   \n",
       "29  JPMorgan cuts Tesla price target, says shares ...  8 Hours Ago   \n",
       "\n",
       "                                            News Link  \n",
       "0   https://www.cnbc.com/2022/07/05/trump-era-chan...  \n",
       "1   https://www.cnbc.com/2022/07/05/why-minions-be...  \n",
       "2   https://www.cnbc.com/2022/07/05/social-securit...  \n",
       "3   https://www.cnbc.com/2022/07/05/ray-dalio-is-h...  \n",
       "4   https://www.cnbc.com/2022/07/05/here-are-top-4...  \n",
       "5   https://www.cnbc.com/2022/07/05/how-investors-...  \n",
       "6   https://www.cnbc.com/2022/07/05/these-stocks-o...  \n",
       "7   https://www.cnbc.com/2022/07/05/georgia-grand-...  \n",
       "8   https://www.cnbc.com/2022/07/05/top-4-recessio...  \n",
       "9   https://www.cnbc.com/2022/07/05/uk-finance-min...  \n",
       "10  https://www.cnbc.com/2022/07/05/one-boring-sto...  \n",
       "11  https://www.cnbc.com/2022/07/05/stocks-making-...  \n",
       "12  https://www.cnbc.com/2022/07/05/bonds-flash-re...  \n",
       "13  https://www.cnbc.com/2022/07/05/luxury-car-buy...  \n",
       "14  https://www.cnbc.com/2022/07/05/goldman-sachs-...  \n",
       "15  https://www.cnbc.com/2022/07/05/investing-club...  \n",
       "16  https://www.cnbc.com/2022/07/05/oil-tumbles-mo...  \n",
       "17  https://www.cnbc.com/2022/07/05/how-to-buy-a-c...  \n",
       "18  https://www.cnbc.com/2022/07/05/italy-declared...  \n",
       "19  https://www.cnbc.com/2022/07/05/bank-of-englan...  \n",
       "20  https://www.cnbc.com/2022/07/05/oil-could-plum...  \n",
       "21  https://www.cnbc.com/2022/07/05/why-driving-bi...  \n",
       "22  https://www.cnbc.com/2022/07/05/germanys-much-...  \n",
       "23  https://www.cnbc.com/2022/07/05/cameron-diaz-j...  \n",
       "24  https://www.cnbc.com/2022/07/05/yum-brands-say...  \n",
       "25  https://www.cnbc.com/2022/07/05/credit-suisse-...  \n",
       "26  https://www.cnbc.com/2022/07/05/buy-crocs-shar...  \n",
       "27  https://www.cnbc.com/2022/07/05/top-chart-anal...  \n",
       "28  https://www.cnbc.com/2022/07/05/ford-f-q2-2022...  \n",
       "29  https://www.cnbc.com/2022/07/05/jpmorgan-cuts-...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "# i) Headline\n",
    "# ii) Time\n",
    "# iii) News Link\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "page = rq.get(url)\n",
    "soup = bs(page.content, 'html.parser')\n",
    "news = soup.find_all('a', class_=\"LatestNews-headline\")\n",
    "latnews = []\n",
    "for i in news:\n",
    "    latnews.append(i.text)\n",
    "times = soup.find_all('time', class_=\"LatestNews-timestamp\")\n",
    "time = []\n",
    "for i in times:\n",
    "    time.append(i.text)\n",
    "links = soup.find_all('a', class_=\"LatestNews-headline\")\n",
    "link = []\n",
    "for i in links:\n",
    "    link.append(i['href'])\n",
    "data = pd.DataFrame()\n",
    "data['HeadLines'] = latnews\n",
    "data['Time'] = time\n",
    "data['News Link'] = link\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beb837a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>Location</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Image URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>3.5</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>World Cafe</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Vibe by The Lalit Traveller,Sector 35, Faridabad</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mad 4 Bar B Que</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Sector 29, Faridabad</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Barbeque 29</td>\n",
       "      <td>North Indian, Mughlai, Desserts, Beverages</td>\n",
       "      <td>NIT, Faridabad</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Glasshouse</td>\n",
       "      <td>European, Italian, Asian, Continental</td>\n",
       "      <td>DoubleTree By Hilton Gurugram Baani Square,Sec...</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Restaurant  \\\n",
       "0                    Castle Barbeque   \n",
       "1                    Jungle Jamboree   \n",
       "2                    Castle Barbeque   \n",
       "3                         Cafe Knosh   \n",
       "4               The Barbeque Company   \n",
       "5                        India Grill   \n",
       "6                     Delhi Barbeque   \n",
       "7   The Monarch - Bar Be Que Village   \n",
       "8                         World Cafe   \n",
       "9                  Indian Grill Room   \n",
       "10                   Mad 4 Bar B Que   \n",
       "11                       Barbeque 29   \n",
       "12                        Glasshouse   \n",
       "\n",
       "                                       Cuisines  \\\n",
       "0                         North Indian, Chinese   \n",
       "1                  North Indian, Asian, Italian   \n",
       "2                         Chinese, North Indian   \n",
       "3                          Italian, Continental   \n",
       "4                         North Indian, Chinese   \n",
       "5                         North Indian, Italian   \n",
       "6                                  North Indian   \n",
       "7                                  North Indian   \n",
       "8                         North Indian, Italian   \n",
       "9                         North Indian, Mughlai   \n",
       "10                                 North Indian   \n",
       "11   North Indian, Mughlai, Desserts, Beverages   \n",
       "12        European, Italian, Asian, Continental   \n",
       "\n",
       "                                             Location Rating  \\\n",
       "0                      Connaught Place, Central Delhi    3.5   \n",
       "1              3CS Mall,Lajpat Nagar - 3, South Delhi    3.9   \n",
       "2              Pacific Mall,Tagore Garden, West Delhi    3.9   \n",
       "3   The Leela Ambience Convention Hotel,Shahdara, ...    4.3   \n",
       "4                  Gardens Galleria,Sector 38A, Noida      4   \n",
       "5                Hilton Garden Inn,Saket, South Delhi    3.9   \n",
       "6      Taurus Sarovar Portico,Mahipalpur, South Delhi    3.7   \n",
       "7   Indirapuram Habitat Centre,Indirapuram, Ghaziabad    3.8   \n",
       "8    Vibe by The Lalit Traveller,Sector 35, Faridabad    4.3   \n",
       "9    Suncity Business Tower,Golf Course Road, Gurgaon    4.3   \n",
       "10                               Sector 29, Faridabad    3.6   \n",
       "11                                     NIT, Faridabad    4.2   \n",
       "12  DoubleTree By Hilton Gurugram Baani Square,Sec...      4   \n",
       "\n",
       "                                            Image URL  \n",
       "0   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "12  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a python program to scrape mentioned details from dineout.co.in :\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests as rq\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants/buffet-special'\n",
    "page = rq.get(url)\n",
    "soup = bs(page.content, 'html.parser')\n",
    "res_names = soup.find_all('div', class_=\"restnt-info cursor\")\n",
    "names = []\n",
    "for name in res_names:\n",
    "    names.append(name.find('a', class_=\"restnt-name ellipsis\").get_text())\n",
    "res_cuisines = soup.find_all('span', class_=\"double-line-ellipsis\")\n",
    "cuisines = []\n",
    "for cuisine in res_cuisines:\n",
    "    cuisines.append(cuisine.get_text().split('|')[1])\n",
    "res_locs = soup.find_all('div', class_=\"restnt-loc ellipsis\")\n",
    "locs = []\n",
    "for loc in res_locs:\n",
    "    locs.append(loc.get_text())\n",
    "res_ratings = soup.find_all('div', class_=\"restnt-rating rating-4\")\n",
    "ratings = []\n",
    "for rating in res_ratings:\n",
    "    ratings.append(rating.get_text())\n",
    "res_urls = soup.find_all('img', class_=\"no-img\")\n",
    "urls = []\n",
    "for url in res_urls:\n",
    "    urls.append(url[\"data-src\"])\n",
    "data = pd.DataFrame()\n",
    "data['Restaurant'] = names\n",
    "data['Cuisines'] = cuisines\n",
    "data['Location'] = locs\n",
    "data['Rating'] = ratings\n",
    "data['Image URL'] = urls\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7a2bbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HeadLines</th>\n",
       "      <th>Time</th>\n",
       "      <th>News Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Judge throws out Trump-era changes that weaken...</td>\n",
       "      <td>29 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/trump-era-chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Minions' vs. 'Lightyear': Here's why the sill...</td>\n",
       "      <td>36 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/why-minions-be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Washington leaders at odds over proposed tax i...</td>\n",
       "      <td>54 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/social-securit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bridgewater's flagship fund is up more than 30...</td>\n",
       "      <td>55 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/ray-dalio-is-h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Here are top 4 and bottom 4 Club stocks in jus...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/here-are-top-4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Here's how to avoid violating ‘wash sale rules...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/how-investors-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>These stocks can offer protection from next bi...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/these-stocks-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Grand jury investigating Trump election interf...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/georgia-grand-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The 4 most 'recession-proof' industries to wor...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/top-4-recessio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UK's Boris Johnson suffers major blow as finan...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/uk-finance-min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>One boring stock quietly hits all-time high as...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/one-boring-sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Stocks making the biggest moves midday: Oil st...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/stocks-making-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bonds flash recession warning as key part of t...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/bonds-flash-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>With record prices and rising loan costs, luxu...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/luxury-car-buy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Goldman Sachs offers three main strategies to ...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/goldman-sachs-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>We're buying more bank shares as the market se...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/investing-club...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Oil tumbles as much as 10%, breaks below $100 ...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/oil-tumbles-mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How to buy a carbon-conscious fund in wake of ...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/how-to-buy-a-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Italy declared a state of emergency because of...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/italy-declared...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bank of England's Bailey warns global economic...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/bank-of-englan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Oil could plummet to the $60s if a recession h...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/oil-could-plum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Why driving big rig trucks is a job fewer Amer...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/why-driving-bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Germany's much-vaunted trade surplus disappear...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/germanys-much-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Cameron Diaz joins 'unretirement' trend—here's...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/cameron-diaz-j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Yum Brands says it is close to selling its Rus...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/yum-brands-say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Credit Suisse slashes year-end S&amp;P 500 target,...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/credit-suisse-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Buy shares of Crocs, which could climb 50% fro...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/buy-crocs-shar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Downside momentum could mean a new low is ahea...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/top-chart-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Ford reports slight uptick in quarterly sales ...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/ford-f-q2-2022...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>JPMorgan cuts Tesla price target, says shares ...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/07/05/jpmorgan-cuts-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            HeadLines         Time  \\\n",
       "0   Judge throws out Trump-era changes that weaken...   29 Min Ago   \n",
       "1   'Minions' vs. 'Lightyear': Here's why the sill...   36 Min Ago   \n",
       "2   Washington leaders at odds over proposed tax i...   54 Min Ago   \n",
       "3   Bridgewater's flagship fund is up more than 30...   55 Min Ago   \n",
       "4   Here are top 4 and bottom 4 Club stocks in jus...   1 Hour Ago   \n",
       "5   Here's how to avoid violating ‘wash sale rules...  2 Hours Ago   \n",
       "6   These stocks can offer protection from next bi...  2 Hours Ago   \n",
       "7   Grand jury investigating Trump election interf...  3 Hours Ago   \n",
       "8   The 4 most 'recession-proof' industries to wor...  3 Hours Ago   \n",
       "9   UK's Boris Johnson suffers major blow as finan...  3 Hours Ago   \n",
       "10  One boring stock quietly hits all-time high as...  3 Hours Ago   \n",
       "11  Stocks making the biggest moves midday: Oil st...  4 Hours Ago   \n",
       "12  Bonds flash recession warning as key part of t...  4 Hours Ago   \n",
       "13  With record prices and rising loan costs, luxu...  5 Hours Ago   \n",
       "14  Goldman Sachs offers three main strategies to ...  5 Hours Ago   \n",
       "15  We're buying more bank shares as the market se...  5 Hours Ago   \n",
       "16  Oil tumbles as much as 10%, breaks below $100 ...  5 Hours Ago   \n",
       "17  How to buy a carbon-conscious fund in wake of ...  5 Hours Ago   \n",
       "18  Italy declared a state of emergency because of...  5 Hours Ago   \n",
       "19  Bank of England's Bailey warns global economic...  6 Hours Ago   \n",
       "20  Oil could plummet to the $60s if a recession h...  6 Hours Ago   \n",
       "21  Why driving big rig trucks is a job fewer Amer...  6 Hours Ago   \n",
       "22  Germany's much-vaunted trade surplus disappear...  6 Hours Ago   \n",
       "23  Cameron Diaz joins 'unretirement' trend—here's...  6 Hours Ago   \n",
       "24  Yum Brands says it is close to selling its Rus...  6 Hours Ago   \n",
       "25  Credit Suisse slashes year-end S&P 500 target,...  7 Hours Ago   \n",
       "26  Buy shares of Crocs, which could climb 50% fro...  7 Hours Ago   \n",
       "27  Downside momentum could mean a new low is ahea...  7 Hours Ago   \n",
       "28  Ford reports slight uptick in quarterly sales ...  7 Hours Ago   \n",
       "29  JPMorgan cuts Tesla price target, says shares ...  8 Hours Ago   \n",
       "\n",
       "                                            News Link  \n",
       "0   https://www.cnbc.com/2022/07/05/trump-era-chan...  \n",
       "1   https://www.cnbc.com/2022/07/05/why-minions-be...  \n",
       "2   https://www.cnbc.com/2022/07/05/social-securit...  \n",
       "3   https://www.cnbc.com/2022/07/05/ray-dalio-is-h...  \n",
       "4   https://www.cnbc.com/2022/07/05/here-are-top-4...  \n",
       "5   https://www.cnbc.com/2022/07/05/how-investors-...  \n",
       "6   https://www.cnbc.com/2022/07/05/these-stocks-o...  \n",
       "7   https://www.cnbc.com/2022/07/05/georgia-grand-...  \n",
       "8   https://www.cnbc.com/2022/07/05/top-4-recessio...  \n",
       "9   https://www.cnbc.com/2022/07/05/uk-finance-min...  \n",
       "10  https://www.cnbc.com/2022/07/05/one-boring-sto...  \n",
       "11  https://www.cnbc.com/2022/07/05/stocks-making-...  \n",
       "12  https://www.cnbc.com/2022/07/05/bonds-flash-re...  \n",
       "13  https://www.cnbc.com/2022/07/05/luxury-car-buy...  \n",
       "14  https://www.cnbc.com/2022/07/05/goldman-sachs-...  \n",
       "15  https://www.cnbc.com/2022/07/05/investing-club...  \n",
       "16  https://www.cnbc.com/2022/07/05/oil-tumbles-mo...  \n",
       "17  https://www.cnbc.com/2022/07/05/how-to-buy-a-c...  \n",
       "18  https://www.cnbc.com/2022/07/05/italy-declared...  \n",
       "19  https://www.cnbc.com/2022/07/05/bank-of-englan...  \n",
       "20  https://www.cnbc.com/2022/07/05/oil-could-plum...  \n",
       "21  https://www.cnbc.com/2022/07/05/why-driving-bi...  \n",
       "22  https://www.cnbc.com/2022/07/05/germanys-much-...  \n",
       "23  https://www.cnbc.com/2022/07/05/cameron-diaz-j...  \n",
       "24  https://www.cnbc.com/2022/07/05/yum-brands-say...  \n",
       "25  https://www.cnbc.com/2022/07/05/credit-suisse-...  \n",
       "26  https://www.cnbc.com/2022/07/05/buy-crocs-shar...  \n",
       "27  https://www.cnbc.com/2022/07/05/top-chart-anal...  \n",
       "28  https://www.cnbc.com/2022/07/05/ford-f-q2-2022...  \n",
       "29  https://www.cnbc.com/2022/07/05/jpmorgan-cuts-...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "page = rq.get(url)\n",
    "soup = bs(page.content, 'html.parser')\n",
    "news = soup.find_all('a', class_=\"LatestNews-headline\")\n",
    "latnews = []\n",
    "for i in news:\n",
    "    latnews.append(i.text)\n",
    "times = soup.find_all('time', class_=\"LatestNews-timestamp\")\n",
    "time = []\n",
    "for i in times:\n",
    "    time.append(i.text)\n",
    "links = soup.find_all('a', class_=\"LatestNews-headline\")\n",
    "link = []\n",
    "for i in links:\n",
    "    link.append(i['href'])\n",
    "data = pd.DataFrame()\n",
    "data['HeadLines'] = latnews\n",
    "data['Time'] = time\n",
    "data['News Link'] = link\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f720e331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Paper URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paper Title  \\\n",
       "0                                    Reward is enough   \n",
       "1                           Making sense of raw input   \n",
       "2   Law and logic: A review from an argumentation ...   \n",
       "3              Creativity and artificial intelligence   \n",
       "4   Artificial cognition for social human–robot in...   \n",
       "5   Explanation in artificial intelligence: Insigh...   \n",
       "6                       Making sense of sensory input   \n",
       "7   Conflict-based search for optimal multi-agent ...   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...   \n",
       "9   The Hanabi challenge: A new frontier for AI re...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11           Argumentation in artificial intelligence   \n",
       "12  Algorithms for computing strategies in two-pla...   \n",
       "13      Multiple object tracking: A literature review   \n",
       "14  Selection of relevant features and examples in...   \n",
       "15  A survey of inverse reinforcement learning: Ch...   \n",
       "16  Explaining individual predictions when feature...   \n",
       "17  A review of possible effects of cognitive bias...   \n",
       "18  Integrating social power into the decision-mak...   \n",
       "19  “That's (not) the output I expected!” On the r...   \n",
       "20  Explaining black-box classifiers using post-ho...   \n",
       "21  Algorithm runtime prediction: Methods & evalua...   \n",
       "22              Wrappers for feature subset selection   \n",
       "23  Commonsense visual sensemaking for autonomous ...   \n",
       "24         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                              Authors  Published Date  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "2                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
       "3                                 Boden, Margaret A.      August 1998   \n",
       "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "5                                        Miller, Tim    February 2019   \n",
       "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "11               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
       "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "14                      Blum, Avrim L., Langley, Pat    December 1997   \n",
       "15                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
       "16      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
       "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
       "19                      Riveiro, Maria, Thill, Serge   September 2021   \n",
       "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "22                      Kohavi, Ron, John, George H.    December 1997   \n",
       "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "24                                   Ying, Mingsheng    February 2010   \n",
       "\n",
       "                                            Paper URL  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded- articles\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests as rq\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "page = rq.get(url)\n",
    "soup = bs(page.content, 'html.parser')\n",
    "scraped_titles = soup.find_all('h2', class_=\"sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR\")\n",
    "titles = []\n",
    "for title in scraped_titles:\n",
    "    titles.append(title.text)\n",
    "scraped_authors = soup.find_all('span', class_=\"sc-1w3fpd7-0 pgLAT\")\n",
    "authors = []\n",
    "for author in scraped_authors:\n",
    "    authors.append(author.text)\n",
    "scraped_dates = soup.find_all('span', class_=\"sc-1thf9ly-2 bKddwo\")\n",
    "dates = []\n",
    "for date in scraped_dates:\n",
    "    dates.append(date.find('span').text)\n",
    "scraped_urls = soup.find_all('li', class_=\"sc-9zxyh7-1 sc-9zxyh7-2 exAXfr jQmQZp\")\n",
    "urls = []\n",
    "for url in scraped_urls:\n",
    "    urls.append(url.find('a')['href'])\n",
    "data = pd.DataFrame()\n",
    "data['Paper Title'] = titles\n",
    "data['Authors'] = authors\n",
    "data['Published Date'] = dates\n",
    "data['Paper URL'] = urls\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "686ed0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Publication</th>\n",
       "      <th>h5-index</th>\n",
       "      <th>h5-median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Nature</td>\n",
       "      <td>444</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>432</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Science</td>\n",
       "      <td>401</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>389</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>354</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.</td>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.</td>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>145</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>145</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.</td>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>144</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>144</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                                        Publication h5-index h5-median\n",
       "0     1.                                             Nature      444       667\n",
       "1     2.                The New England Journal of Medicine      432       780\n",
       "2     3.                                            Science      401       614\n",
       "3     4.  IEEE/CVF Conference on Computer Vision and Pat...      389       627\n",
       "4     5.                                         The Lancet      354       635\n",
       "..   ...                                                ...      ...       ...\n",
       "95   96.                       Journal of Business Research      145       233\n",
       "96   97.                                   Molecular Cancer      145       209\n",
       "97   98.                                            Sensors      145       201\n",
       "98   99.                              Nature Climate Change      144       228\n",
       "99  100.                    IEEE Internet of Things Journal      144       212\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a python program to scrape the details of top publications from Google Scholar fromhttps://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests as rq\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://scholar.google.com/citations?view_op=top_venues&hl=en'\n",
    "page = rq.get(url)\n",
    "soup = bs(page.content, 'html.parser')\n",
    "scr_ranks = soup.find_all('td', class_=\"gsc_mvt_p\")\n",
    "ranks = []\n",
    "for rank in scr_ranks:\n",
    "    ranks.append(rank.get_text())\n",
    "scr_publs = soup.find_all('td', class_=\"gsc_mvt_t\")\n",
    "publs = []\n",
    "for publ in scr_publs:\n",
    "    publs.append(publ.get_text())\n",
    "scr_inds = soup.find_all('a', class_=\"gs_ibl gsc_mp_anchor\")\n",
    "scr_inds\n",
    "index = []\n",
    "for ind in scr_inds:\n",
    "    index.append(ind.get_text())\n",
    "scr_meds = soup.find_all('span', class_=\"gs_ibl gsc_mp_anchor\")\n",
    "median = []\n",
    "for med in scr_meds:\n",
    "    median.append(med.text)\n",
    "data = pd.DataFrame()\n",
    "data['Rank'] = ranks\n",
    "data['Publication'] = publs\n",
    "data['h5-index'] = index\n",
    "data['h5-median'] = median\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d9da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
